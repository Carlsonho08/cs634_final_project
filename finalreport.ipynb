{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Chi-square Test\n",
    "You can use the Chi-square test to analyze the independence or association between categorical variables, such as property location (Place) and property type (e.g., description). This can help you understand if certain property types are more common in specific areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a contingency table (cross-tabulation) for the Chi-square test\n",
    "contingency_table = pd.crosstab(df['Place'], df['Description'])\n",
    "\n",
    "# Print the contingency table\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Perform the Chi-square test\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nChi-square Statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "print(\"\\nSignificance level (alpha):\", alpha)\n",
    "print(\"Conclusion:\")\n",
    "if p < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant association between property location and property type.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant association between property location and property type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Data Similarity and Dissimilarity\n",
    "You can measure data similarity and dissimilarity between properties using techniques like cosine similarity. This can help identify similar properties in the dataset based on their feature vectors, providing insights into property trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # Add LabelEncoder here\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for similarity analysis\n",
    "similarity_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "\n",
    "# Convert categorical variables into numerical format for similarity analysis\n",
    "label_encoder = LabelEncoder()\n",
    "similarity_data['Place'] = label_encoder.fit_transform(similarity_data['Place'])\n",
    "similarity_data['Description'] = label_encoder.fit_transform(similarity_data['Description'])\n",
    "\n",
    "# Standardize the features for similarity analysis\n",
    "scaler = StandardScaler()\n",
    "similarity_data_scaled = scaler.fit_transform(similarity_data[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']])\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(similarity_data_scaled, similarity_data_scaled)\n",
    "\n",
    "# Display the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim_matrix)\n",
    "\n",
    "# Example: Find similar properties to a given property (e.g., first property in the dataset)\n",
    "property_index = 0\n",
    "similar_properties = cosine_sim_matrix[property_index]\n",
    "\n",
    "# Display the most similar properties\n",
    "print(\"\\nMost Similar Properties to Property at Index\", property_index)\n",
    "similar_properties_indices = sorted(range(len(similar_properties)), key=lambda i: similar_properties[i], reverse=True)[1:6]\n",
    "for index in similar_properties_indices:\n",
    "    print(f\"Property at Index {index}: Cosine Similarity = {similar_properties[index]}\")\n",
    "    print(df.loc[index, ['Address', 'Price', 'Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent Itemsets and Compact Representation\n",
    "Mining frequent itemsets can reveal common combinations of property features. You can identify common property feature patterns and compactly represent these patterns to gain insights into which combinations are prevalent in the market.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80328b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for frequent itemset mining\n",
    "itemset_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "\n",
    "# Convert categorical variables into numerical format for frequent itemset mining\n",
    "label_encoder = LabelEncoder()\n",
    "itemset_data['Place'] = label_encoder.fit_transform(itemset_data['Place'])\n",
    "itemset_data['Description'] = label_encoder.fit_transform(itemset_data['Description'])\n",
    "\n",
    "# Convert the dataset to a one-hot encoded format\n",
    "one_hot_encoded = pd.get_dummies(itemset_data, columns=['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description'])\n",
    "\n",
    "# Apriori to find frequent itemsets\n",
    "frequent_itemsets = apriori(one_hot_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Display the generated frequent itemsets\n",
    "print(\"Generated Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Compact representation: Keep only relevant information\n",
    "compact_representation = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
    "\n",
    "# Display the compact representation\n",
    "print(\"\\nCompact Representation of Frequent Itemsets:\")\n",
    "print(compact_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means\n",
    "K-means clustering can help you group properties based on their features. For example, you can cluster properties into different segments based on factors like the number of bedrooms, bathrooms, square footage, and price. This can reveal patterns in the market and identify different market segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['Price', 'Beds', 'Bath', 'Sq.Ft']]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Determine the number of clusters (you can adjust this based on your requirements)\n",
    "num_clusters = 3\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Display the clustered properties\n",
    "print(\"Clustered Properties:\")\n",
    "print(df[['Address', 'Price', 'Beds', 'Bath', 'Sq.Ft', 'Cluster']])\n",
    "\n",
    "# Visualize the clusters (for 2D visualization)\n",
    "plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=df['Cluster'], cmap='viridis')\n",
    "plt.xlabel('Standardized Price')\n",
    "plt.ylabel('Standardized Beds')\n",
    "plt.title('K-means Clustering of Properties')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN (Nearest Neighbor Classifiers)\n",
    "K-nearest neighbor classifiers can be applied to predict housing prices or property types based on similar properties. Given a property's features, you can find the most similar properties in the dataset and use their prices or types as predictions for the target property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for KNN\n",
    "knn_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "\n",
    "# Convert categorical variables into numerical format for KNN\n",
    "label_encoder = LabelEncoder()\n",
    "knn_data['Place'] = label_encoder.fit_transform(knn_data['Place'])\n",
    "knn_data['Description'] = label_encoder.fit_transform(knn_data['Description'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = knn_data[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "y_price = df['Price']\n",
    "y_type = df['Description']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_price_train, y_price_test, y_type_train, y_type_test = train_test_split(\n",
    "    X, y_price, y_type, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# KNN for predicting housing prices\n",
    "knn_price = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_price.fit(X_train, y_price_train)\n",
    "y_price_pred = knn_price.predict(X_test)\n",
    "\n",
    "# Evaluate the KNN model for housing prices\n",
    "mse_price = mean_squared_error(y_price_test, y_price_pred)\n",
    "print(\"\\nKNN Model for Housing Prices:\")\n",
    "print(\"Mean Squared Error:\", mse_price)\n",
    "\n",
    "# KNN for predicting property types\n",
    "knn_type = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_type.fit(X_train, y_type_train)\n",
    "y_type_pred = knn_type.predict(X_test)\n",
    "\n",
    "# Evaluate the KNN model for property types\n",
    "accuracy_type = accuracy_score(y_type_test, y_type_pred)\n",
    "print(\"\\nKNN Model for Property Types:\")\n",
    "print(\"Accuracy Score:\", accuracy_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule Generation\n",
    "You can generate association rules to discover patterns and relationships between property features. For example, you can find rules like \"In area X, properties with more bedrooms tend to have higher prices.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for rule generation\n",
    "rule_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description', 'Price']]\n",
    "\n",
    "# Convert categorical variables into numerical format for rule generation\n",
    "label_encoder = LabelEncoder()\n",
    "rule_data['Place'] = label_encoder.fit_transform(rule_data['Place'])\n",
    "rule_data['Description'] = label_encoder.fit_transform(rule_data['Description'])\n",
    "\n",
    "# Bin the continuous variable (Price) to create categorical labels\n",
    "bins = [0, 500000, 1000000, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "rule_data['Price_Category'] = pd.cut(rule_data['Price'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Drop the original Price column\n",
    "rule_data = rule_data.drop('Price', axis=1)\n",
    "\n",
    "# Convert the dataset to a one-hot encoded format\n",
    "one_hot_encoded = pd.get_dummies(rule_data, columns=['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description', 'Price_Category'])\n",
    "\n",
    "# Apriori to find frequent itemsets\n",
    "frequent_itemsets = apriori(one_hot_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Display the generated association rules\n",
    "print(\"Generated Association Rules:\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "You can use TF-IDF to analyze textual data, such as property descriptions. This can help identify keywords or terms that are more prevalent in certain property descriptions and gain insights into market trends based on textual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract property descriptions\n",
    "descriptions = df['Description']\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "# Fit and transform the property descriptions\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# Example: Display the top keywords for each property description\n",
    "for i, description in enumerate(descriptions):\n",
    "    print(f\"\\nTop keywords for Property {i + 1}: {description}\")\n",
    "    keywords_indices = tfidf_df.iloc[i].sort_values(ascending=False).head(5).index\n",
    "    print(tfidf_vectorizer.get_feature_names_out()[keywords_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees and ensemble methods like random forests can be used for regression tasks to predict housing prices. These methods are interpretable and can capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features for regression\n",
    "features = df[['Beds', 'Bath', 'Sq.Ft']]\n",
    "\n",
    "# Target variable\n",
    "target = df['Price']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Decision Tree Regression\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_dt_pred = dt_regressor.predict(X_test)\n",
    "\n",
    "# Random Forest Regression\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_rf_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_dt = mean_squared_error(y_test, y_dt_pred)\n",
    "r2_dt = r2_score(y_test, y_dt_pred)\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_rf_pred)\n",
    "r2_rf = r2_score(y_test, y_rf_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Decision Tree Regression:\")\n",
    "print(\"Mean Squared Error:\", mse_dt)\n",
    "print(\"R-squared:\", r2_dt)\n",
    "\n",
    "print(\"\\nRandom Forest Regression:\")\n",
    "print(\"Mean Squared Error:\", mse_rf)\n",
    "print(\"R-squared:\", r2_rf)\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(y_test, y_dt_pred, alpha=0.5)\n",
    "plt.title('Decision Tree Regression: Predictions vs. Actual Values')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(y_test, y_rf_pred, alpha=0.5)\n",
    "plt.title('Random Forest Regression: Predictions vs. Actual Values')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier Detection:\n",
    "Identify and handle outliers in your dataset, as they can significantly impact model performance and distort insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features for outlier detection\n",
    "features = df[['Beds', 'Bath', 'Sq.Ft', 'Price']]\n",
    "\n",
    "# Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)  # Adjust contamination based on your dataset\n",
    "outlier_labels = isolation_forest.fit_predict(features)\n",
    "\n",
    "# Add outlier labels to the original DataFrame\n",
    "df['Outlier'] = outlier_labels\n",
    "\n",
    "# Display the properties and their outlier status\n",
    "print(\"Properties and Outlier Status:\")\n",
    "print(df[['Address', 'Beds', 'Bath', 'Sq.Ft', 'Price', 'Outlier']])\n",
    "\n",
    "# Visualize outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Price'], df['Sq.Ft'], c=df['Outlier'], cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Square Footage')\n",
    "plt.title('Outlier Detection: Price vs. Square Footage')\n",
    "plt.colorbar(label='Outlier Status')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machines (GBM):\n",
    "GBM is another ensemble learning technique that builds a series of weak learners sequentially, each correcting the errors of the previous one. It is powerful for regression tasks and can handle non-linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features for regression\n",
    "features = df[['Beds', 'Bath', 'Sq.Ft']]\n",
    "\n",
    "# Target variable\n",
    "target = df['Price']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting Regression\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_gb_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_gb = mean_squared_error(y_test, y_gb_pred)\n",
    "r2_gb = r2_score(y_test, y_gb_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Gradient Boosting Regression:\")\n",
    "print(\"Mean Squared Error:\", mse_gb)\n",
    "print(\"R-squared:\", r2_gb)\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(y_test, y_gb_pred, alpha=0.5)\n",
    "plt.title('Gradient Boosting Regression: Predictions vs. Actual Values')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
