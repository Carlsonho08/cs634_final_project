{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori and FP-growth\n",
    "Apriori and FP-growth are commonly used for market basket analysis, but they can be adapted to analyze real estate market trends. You can use these algorithms to discover associations between different property features or characteristics, such as the number of bedrooms, bathrooms, and location (Place).\n",
    "Error optput but codes are there. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Load the real estate dataset\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select relevant columns for analysis\n",
    "property_data = df[['Beds', 'Bath', 'Place']]\n",
    "\n",
    "# Convert categorical variables into numerical format for Apriori and FP-growth\n",
    "property_data_encoded = pd.get_dummies(property_data, columns=['Beds', 'Bath', 'Place'])\n",
    "\n",
    "# Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets_apriori = apriori(property_data_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# FP-growth algorithm to find frequent itemsets\n",
    "frequent_itemsets_fpgrowth = fpgrowth(property_data_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(\"Frequent Itemsets using Apriori:\")\n",
    "print(frequent_itemsets_apriori)\n",
    "\n",
    "print(\"\\nFrequent Itemsets using FP-growth:\")\n",
    "print(frequent_itemsets_fpgrowth)\n",
    "\n",
    "# Generate association rules using Apriori\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Generate association rules using FP-growth\n",
    "rules_fpgrowth = association_rules(frequent_itemsets_fpgrowth, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Display the generated association rules\n",
    "print(\"\\nAssociation Rules using Apriori:\")\n",
    "print(rules_apriori)\n",
    "\n",
    "print(\"\\nAssociation Rules using FP-growth:\")\n",
    "print(rules_fpgrowth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Chi-square Test\n",
    "You can use the Chi-square test to analyze the independence or association between categorical variables, such as property location (Place) and property type (e.g., description). This can help you understand if certain property types are more common in specific areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a contingency table (cross-tabulation) for the Chi-square test\n",
    "contingency_table = pd.crosstab(df['Place'], df['Description'])\n",
    "\n",
    "# Print the contingency table\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Perform the Chi-square test\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nChi-square Statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "print(\"\\nSignificance level (alpha):\", alpha)\n",
    "print(\"Conclusion:\")\n",
    "if p < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant association between property location and property type.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant association between property location and property type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Data Similarity and Dissimilarity\n",
    "You can measure data similarity and dissimilarity between properties using techniques like cosine similarity. This can help identify similar properties in the dataset based on their feature vectors, providing insights into property trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # Add LabelEncoder here\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for similarity analysis\n",
    "similarity_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "\n",
    "# Convert categorical variables into numerical format for similarity analysis\n",
    "label_encoder = LabelEncoder()\n",
    "similarity_data['Place'] = label_encoder.fit_transform(similarity_data['Place'])\n",
    "similarity_data['Description'] = label_encoder.fit_transform(similarity_data['Description'])\n",
    "\n",
    "# Standardize the features for similarity analysis\n",
    "scaler = StandardScaler()\n",
    "similarity_data_scaled = scaler.fit_transform(similarity_data[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']])\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(similarity_data_scaled, similarity_data_scaled)\n",
    "\n",
    "# Display the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim_matrix)\n",
    "\n",
    "# Example: Find similar properties to a given property (e.g., first property in the dataset)\n",
    "property_index = 0\n",
    "similar_properties = cosine_sim_matrix[property_index]\n",
    "\n",
    "# Display the most similar properties\n",
    "print(\"\\nMost Similar Properties to Property at Index\", property_index)\n",
    "similar_properties_indices = sorted(range(len(similar_properties)), key=lambda i: similar_properties[i], reverse=True)[1:6]\n",
    "for index in similar_properties_indices:\n",
    "    print(f\"Property at Index {index}: Cosine Similarity = {similar_properties[index]}\")\n",
    "    print(df.loc[index, ['Address', 'Price', 'Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent Itemsets and Compact Representation\n",
    "Mining frequent itemsets can reveal common combinations of property features. You can identify common property feature patterns and compactly represent these patterns to gain insights into which combinations are prevalent in the market.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80328b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for frequent itemset mining\n",
    "itemset_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "\n",
    "# Convert categorical variables into numerical format for frequent itemset mining\n",
    "label_encoder = LabelEncoder()\n",
    "itemset_data['Place'] = label_encoder.fit_transform(itemset_data['Place'])\n",
    "itemset_data['Description'] = label_encoder.fit_transform(itemset_data['Description'])\n",
    "\n",
    "# Convert the dataset to a one-hot encoded format\n",
    "one_hot_encoded = pd.get_dummies(itemset_data, columns=['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description'])\n",
    "\n",
    "# Apriori to find frequent itemsets\n",
    "frequent_itemsets = apriori(one_hot_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Display the generated frequent itemsets\n",
    "print(\"Generated Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Compact representation: Keep only relevant information\n",
    "compact_representation = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
    "\n",
    "# Display the compact representation\n",
    "print(\"\\nCompact Representation of Frequent Itemsets:\")\n",
    "print(compact_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means\n",
    "K-means clustering can help you group properties based on their features. For example, you can cluster properties into different segments based on factors like the number of bedrooms, bathrooms, square footage, and price. This can reveal patterns in the market and identify different market segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['Price', 'Beds', 'Bath', 'Sq.Ft']]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Determine the number of clusters (you can adjust this based on your requirements)\n",
    "num_clusters = 3\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Display the clustered properties\n",
    "print(\"Clustered Properties:\")\n",
    "print(df[['Address', 'Price', 'Beds', 'Bath', 'Sq.Ft', 'Cluster']])\n",
    "\n",
    "# Visualize the clusters (for 2D visualization)\n",
    "plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=df['Cluster'], cmap='viridis')\n",
    "plt.xlabel('Standardized Price')\n",
    "plt.ylabel('Standardized Beds')\n",
    "plt.title('K-means Clustering of Properties')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN (Nearest Neighbor Classifiers)\n",
    "K-nearest neighbor classifiers can be applied to predict housing prices or property types based on similar properties. Given a property's features, you can find the most similar properties in the dataset and use their prices or types as predictions for the target property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for KNN\n",
    "knn_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "\n",
    "# Convert categorical variables into numerical format for KNN\n",
    "label_encoder = LabelEncoder()\n",
    "knn_data['Place'] = label_encoder.fit_transform(knn_data['Place'])\n",
    "knn_data['Description'] = label_encoder.fit_transform(knn_data['Description'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = knn_data[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description']]\n",
    "y_price = df['Price']\n",
    "y_type = df['Description']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_price_train, y_price_test, y_type_train, y_type_test = train_test_split(\n",
    "    X, y_price, y_type, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# KNN for predicting housing prices\n",
    "knn_price = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_price.fit(X_train, y_price_train)\n",
    "y_price_pred = knn_price.predict(X_test)\n",
    "\n",
    "# Evaluate the KNN model for housing prices\n",
    "mse_price = mean_squared_error(y_price_test, y_price_pred)\n",
    "print(\"\\nKNN Model for Housing Prices:\")\n",
    "print(\"Mean Squared Error:\", mse_price)\n",
    "\n",
    "# KNN for predicting property types\n",
    "knn_type = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_type.fit(X_train, y_type_train)\n",
    "y_type_pred = knn_type.predict(X_test)\n",
    "\n",
    "# Evaluate the KNN model for property types\n",
    "accuracy_type = accuracy_score(y_type_test, y_type_pred)\n",
    "print(\"\\nKNN Model for Property Types:\")\n",
    "print(\"Accuracy Score:\", accuracy_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Warehouse & OLAP\n",
    "You can create a data warehouse and use Online Analytical Processing (OLAP) techniques to perform multidimensional analysis of the data. This can help you drill down into different dimensions (e.g., location, property type, and price range) to gain insights into the real estate market from different angles.\n",
    "Have not implemented the second dataset into it yet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a SQLite database and engine\n",
    "db_path = \"real_estate_database.db\"  # Replace with the desired path for the database file\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "# Save the DataFrame to the database\n",
    "df.to_sql('real_estate', engine, index=False, if_exists='replace')\n",
    "\n",
    "# OLAP Analysis Example: Average Price by Location and Property Type\n",
    "query = \"\"\"\n",
    "    SELECT Place, Description, AVG(Price) as AvgPrice\n",
    "    FROM real_estate\n",
    "    GROUP BY Place, Description\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and fetch results\n",
    "result_df = pd.read_sql(query, engine)\n",
    "\n",
    "# Display the OLAP results\n",
    "print(\"OLAP Analysis - Average Price by Location and Property Type:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule Generation\n",
    "You can generate association rules to discover patterns and relationships between property features. For example, you can find rules like \"In area X, properties with more bedrooms tend to have higher prices.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns for rule generation\n",
    "rule_data = df[['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description', 'Price']]\n",
    "\n",
    "# Convert categorical variables into numerical format for rule generation\n",
    "label_encoder = LabelEncoder()\n",
    "rule_data['Place'] = label_encoder.fit_transform(rule_data['Place'])\n",
    "rule_data['Description'] = label_encoder.fit_transform(rule_data['Description'])\n",
    "\n",
    "# Bin the continuous variable (Price) to create categorical labels\n",
    "bins = [0, 500000, 1000000, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "rule_data['Price_Category'] = pd.cut(rule_data['Price'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Drop the original Price column\n",
    "rule_data = rule_data.drop('Price', axis=1)\n",
    "\n",
    "# Convert the dataset to a one-hot encoded format\n",
    "one_hot_encoded = pd.get_dummies(rule_data, columns=['Beds', 'Bath', 'Sq.Ft', 'Place', 'Description', 'Price_Category'])\n",
    "\n",
    "# Apriori to find frequent itemsets\n",
    "frequent_itemsets = apriori(one_hot_encoded, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Display the generated association rules\n",
    "print(\"Generated Association Rules:\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpson's Paradox\n",
    "This statistical paradox can help you uncover hidden trends or patterns in the data. You can apply Simpson's Paradox to analyze how trends change when properties are grouped or split based on specific characteristics, such as location or property type.\n",
    "Error optput but codes are there. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b070aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a simplified dataset for illustration\n",
    "# You may need to adapt this to your actual dataset and analysis\n",
    "simplified_df = df[['Place', 'Beds', 'Price']]\n",
    "\n",
    "# Group by 'Place' and calculate average price\n",
    "average_price_by_place = simplified_df.groupby('Place')['Price'].mean().reset_index()\n",
    "\n",
    "# Plot the overall average price\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='Place', y='Price', data=average_price_by_place, color='lightblue')\n",
    "plt.title('Overall Average Price by Placce')\n",
    "plt.show()\n",
    "\n",
    "# Apply Simpson's Paradox by introducing a confounding variable ('Beds')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Place', y='Price', hue='Beds', data=simplified_df, palette='viridis')\n",
    "plt.title('Average Price by Place and Number of Bedrooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "You can use TF-IDF to analyze textual data, such as property descriptions. This can help identify keywords or terms that are more prevalent in certain property descriptions and gain insights into market trends based on textual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"datasets\\Homes for Sale and Real Estate.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract property descriptions\n",
    "descriptions = df['Description']\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "# Fit and transform the property descriptions\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# Example: Display the top keywords for each property description\n",
    "for i, description in enumerate(descriptions):\n",
    "    print(f\"\\nTop keywords for Property {i + 1}: {description}\")\n",
    "    keywords_indices = tfidf_df.iloc[i].sort_values(ascending=False).head(5).index\n",
    "    print(tfidf_vectorizer.get_feature_names_out()[keywords_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
